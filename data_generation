#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 16 12:56:33 2019

@author: nereabejar
"""

import random
import pandas as pd

##########################################################################################
#######################           DATASET CREATION          ##############################
##########################################################################################
# Generate a randomised data set of Koodoo survival statistics

data = {
    'name': [],
    'coordinate_x': [],
    'coordinate_y': [],
    'age': [],
    'horns_length': [],
    'height': [],
    'weight': [],
    'fur_color': []
}
ds = pd.DataFrame(data)

for n in range(1000):
 
    name = 'Kenny_' + str(n)
    coordinate_x = random.randrange(0, 100, 1)
    coordinate_y = random.randrange(0, 100, 1)
    age = random.randrange(1, 10, 1)
    horns_length = random.randrange(10, 40, 1)
    height = random.randrange(120, 190, 1)
    weight = random.randrange(175, 300, 1)
    fur_color = 'Light Brown' if random.randrange(0, 2,
                                                  1) == 1 else 'Dark Brown'

    data_row = {
        'name': [name],
        'coordinate_x': [coordinate_x],
        'coordinate_y': [coordinate_y],
        'age': [age],
        'horns_length': [horns_length],
        'height': [height],
        'weight': [weight],
        'fur_color': [fur_color]
    }

    row = pd.DataFrame(data_row)
    ds = ds.append(row, 'sort=False')

print(ds)


def DT_labels_creator(df):
    hl_thres=30
    age_thres=5
    w_thres=250
    for nam in df.name.unique():
        if(df[df.name==nam]['coordinate_x'].iloc[0]<10):
            if(df[df.name==nam]['horns_length'].iloc[0]<hl_thres):
                lab='Died'
            else:
                lab='Survived'
        elif(df[df.name==nam]['coordinate_x'].iloc[0]<35):
            if((df[df.name==nam]['coordinate_y'].iloc[0]<20) | (df[df.name==nam]['coordinate_y'].iloc[0]>70)):
                if(df[df.name==nam]['horns_length'].iloc[0]<hl_thres):
                    lab='Died'
                else:
                    lab='Survived'
            else:
                lab='Survived'
        elif(df[df.name==nam]['coordinate_x'].iloc[0]<55):
            if(df[df.name==nam]['coordinate_y'].iloc[0]<20):
                if(df[df.name==nam]['horns_length'].iloc[0]<hl_thres):
                    lab='Died'
                elif(df[df.name==nam]['age'].iloc[0]<age_thres):
                    lab='Survived'
                elif(df[df.name==nam]['fur_color'].iloc[0]=='Light Brown'):
                    lab='Survived'
                else:
                    lab='Died'
            else:
                if(df[df.name==nam]['age'].iloc[0]<age_thres):
                    lab='Survived'
                else:
                    lab='Died'
        elif(df[df.name==nam]['coordinate_x'].iloc[0]<80):
            if(df[df.name==nam]['coordinate_y'].iloc[0]<20):
                if(df[df.name==nam]['weight'].iloc[0]>w_thres):
                    lab='Died'
                elif(df[df.name==nam]['age'].iloc[0]<age_thres):
                    lab='Survived'
                elif(df[df.name==nam]['fur_color'].iloc[0]=='Light Brown'):
                    lab='Survived'
                else:
                    lab='Died'
            elif(df[df.name==nam]['coordinate_y'].iloc[0]<60):
                if(df[df.name==nam]['weight'].iloc[0]>w_thres):
                    lab='Died'
                elif(df[df.name==nam]['age'].iloc[0]<age_thres):
                    lab='Survived'
                else:
                    lab='Died'
            else:
                lab='Died'
        else:
            if(df[df.name==nam]['coordinate_y'].iloc[0]<30):
                if(df[df.name==nam]['fur_color'].iloc[0]=='Light Brown'):
                    lab='Survived'
                else:
                    lab='Died'
            elif(df[df.name==nam]['coordinate_y'].iloc[0]<60):
                if(df[df.name==nam]['age'].iloc[0]<age_thres):
                    lab='Survived'
                else:
                    lab='Died'
            else:
                lab='Died'
        df.loc[df.name==nam,'fate']=lab
        
ds['fate'] = ""
DT_labels_creator(ds)
print(ds)


ds.to_csv('Koodoo_survival_statistics.csv')

##########################################################################################
############################           MODELS          ###################################
##########################################################################################

ds = pd.read_csv('Koodoo_survival_statistics.csv')

from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import train_test_split #to easily split data in train and test
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import matplotlib.pyplot as plt
import numpy as np

#######---------- auxuliar functions --------------------
def kfold_cross_validation(df,tree,folds_num,features,target):
    kf = KFold(n_splits = folds_num, shuffle=True)
    attributes = df[features]
    labels = df[target]
    accuracy= []
    precision = []
    recall = []
    f1score =[]
    #RMSE = []
    scores= []
    for i in range(folds_num):
        result = next(kf.split(attributes), None)
        x_train = attributes.iloc[result[0]]
        x_test = attributes.iloc[result[1]]
        y_train = labels.iloc[result[0]]
        y_test = labels.iloc[result[1]]
        model = tree.fit(x_train,y_train)
        y_pred = tree.predict(x_test)
        accuracy.append(accuracy_score(y_test, y_pred))
        precision.append(precision_score(y_test, y_pred, average="weighted"))
        recall.append(recall_score(y_test, y_pred, average="weighted"))
        f1score.append(f1_score(y_test, y_pred, average="weighted"))
    scores = [np.mean(accuracy),np.mean(precision), np.mean(recall),np.mean(f1score)]
    return(scores)

def DT_param_tunning_classification(df,features,target):
    max_score = 0
    score = 0
    criterion = ["gini","entropy"]
    splitter = ["best", "random"]
    max_depth = [None,2,3,4,5,6,7,8,9,10]
    min_samples_split = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,22,24,26,28,30]
    min_samples_leaf = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,18,20]
    max_features = [None,"auto","sqrt","log2"]
    for crit in criterion:
        for split in splitter:
            for depth in max_depth:
                for min_split in min_samples_split:
                    for min_leaf in min_samples_leaf:
                        for feat in max_features:
                            tree = DecisionTreeClassifier(criterion =crit ,splitter=split,max_depth=depth,min_samples_split=min_split,min_samples_leaf=min_leaf,max_features=feat)
                            print("Criterion: "+str(crit)+", Splitter: "+str(split)+", max_depth: "+str(depth)+", min_samples_split: "+str(min_split)+", min_samples_leaf: "+str(min_leaf)+", max_features: "+str(feat))
                            score = kfold_cross_validation(df,tree,10,features,target)
                            #result="Criterion:,"+str(crit)+", Splitter:,"+str(split)+", max_depth:,"+str(depth)+", min_samples_split:,"+str(min_split)+", min_samples_leaf:,"+str(min_leaf)+", max_features:,"+str(feat)+",score:,"+str(score)+",\n"
                            result="Criterion:,"+str(crit)+", Splitter:,"+str(split)+", max_depth:,"+str(depth)+", min_samples_split:,"+str(min_split)+", min_samples_leaf:,"+str(min_leaf)+", max_features:,"+str(feat)+",accuracy:,"+str(score[0])+",precision:,"+str(score[1])+",recall:,"+str(score[2])+",f1score:,"+str(score[3])+",\n"
                            file1 = open("DTClass_ParamTunning.txt","a") #Append Only (‘a’) : Open the file for writing. The file is created if it does not exist. The handle is positioned at the end of the file. The data being written will be inserted at the end, after the existing data.
                            file1.write(result)
                            file1.close()
                            #if(score>max_score):
                            if(score[0]>max_score):#here we use accuracy, but we can change it for example to f1score using score[3]
                                max_score=score[0]
                                max_score_param = "Criterion: "+str(crit)+", Splitter: "+str(split)+", max_depth: "+str(depth)+", min_samples_split: "+str(min_split)+", min_samples_leaf: "+str(min_leaf)+", max_features: "+str(feat)
    best = [max_score,max_score_param]
    return(best)


#-------------------------------- Preparing the data ----------------
    
#Transforming categorical attributes:
fate_to_num = {"fate":     {"Died": 0, "Survived": 1}}
ds.replace(fate_to_num, inplace=True)
fur_color_to_num = {"fur_color":     {"Light Brown": 1, "Dark Brown": 0}}
ds.replace(fur_color_to_num, inplace=True)

#defining features
features = ['coordinate_x','coordinate_y','age','horns_length', 'height','weight','fur_color'] 

#splitting data in training and test set
ds_train, ds_test = train_test_split(ds, test_size=0.15)

#------------------------------------------------------------------------
#-----------------------------Decision Tree -----------------------------
#------------------------------------------------------------------------

#parameter tunning
best = DT_param_tunning_classification(ds,features,"fate")
#[0.9729999999999999,
#'Criterion: gini, Splitter: best, max_depth: None, min_samples_split: 5, min_samples_leaf: 1, max_features: None']

#creating the final tree with the parameters obtained from parameter tunning
tree_final = DecisionTreeClassifier(criterion = 'gini' ,splitter= 'best', max_depth=None, min_samples_split=5, min_samples_leaf=1,max_features=None)

x_train = ds_train[features]
y_train = ds_train["fate"]

x_test = ds_test[features]
y_test = ds_test["fate"]

model = tree_final.fit(x_train, y_train) #training
y_pred = tree_final.predict(x_test)

#...........................DT: Perfomance measures .....................

print("Accuracy:")
accuracy_score(y_test, y_pred)
#0.94
print("Precision:")
precision_score(y_test, y_pred, average="weighted") #labels=np.unique(y_pred) can be added to calculate the measure only for the labels that have predicted samples
#0.9398905866817582

print("Recall:")
recall_score(y_test, y_pred, average="weighted")
#0.94

print("F1-Score:")
f1_score(y_test, y_pred, average="weighted")
# 0.9399012925969447

#...........................DT: Graphical representation .....................
import pydotplus
from scipy import misc
import io

def show_tree(tree, features, path):
    f = io.StringIO() #This creates an object StringBuffer which is quite similar to
    #a string but, while a string isa fixed-length, immutable character sequences and this is
    #growable and writable character sequences. In this case the object is empty.
    export_graphviz(tree, out_file = f, feature_names=features)
    pydotplus.graph_from_dot_data(f.getvalue()).write_png(path)
    img=misc.imread(path)
    plt.rcParams["figure.figsize"]=(20,20)
    plt.imshow(img)

show_tree(tree_final, features,'dec_tree.png')

#...........................DT: Saving the model .....................
import pickle
pickle.dump(tree_final, open('CT_class_model.sav', 'wb'))
# load the model from disk
#tree_final = pickle.load(open(filename, 'rb')) 


#------------------------------------------------------------------------
#-------------------------Decision Tree with Grid Search-----------------
#------------------------------------------------------------------------
from sklearn.model_selection import GridSearchCV

        
tree_GS = DecisionTreeClassifier()
parameters={'min_samples_split' : range(18,500,10),'max_depth': range(1,20,2) ,'criterion': ["gini"],
            'splitter': ["best"],'min_samples_leaf': range(1,20,2)}
#parameters={'min_samples_split' : range(18,500,10),'max_depth': range(1,20,2)}
best = GridSearchCV(tree_GS,parameters)
best.fit(x_train, y_train)
y_pred_DT_GS = best.predict(x_test)

#...........................DT GS: Perfomance measures .....................

print("Accuracy:")
accuracy_score(y_test, y_pred_DT_GS)
#0.9333333333333333
print("Precision:")
precision_score(y_test, y_pred_DT_GS, average="weighted") #labels=np.unique(y_pred) can be added to calculate the measure only for the labels that have predicted samples
#0.9374031007751938
print("Recall:")
recall_score(y_test, y_pred_DT_GS, average="weighted")
#0.9333333333333333
print("F1-Score:")
f1_score(y_test, y_pred_DT_GS, average="weighted")
#0.9338490820900105


#---------------------------------------------------------------------------------
#-------------------------------------Random Forests------------------------------
#---------------------------------------------------------------------------------
from sklearn.ensemble import RandomForestClassifier


def RF_param_tunning(df,features,target):
    max_score = 0
    score = 0
    criterion = ["gini"]
    #max_depth = [None,2,3,4,5,6,7,8,9,10]
    max_depth = [5,10]
    min_samples_split = range(2,20,2)
    min_samples_leaf = range(2,20,2)
    max_features = [None]
    #obs_score = [True,False]
    n_estimators= [50,100,150]
    for n_est in n_estimators:
        for depth in max_depth:
            for min_split in min_samples_split:
                for min_leaf in min_samples_leaf:
                    for feat in max_features:
                        for crit in criterion:
                            #for obs in obs_score:
                            ran_for = RandomForestClassifier(n_estimators= n_est, criterion =crit ,max_depth=depth,min_samples_split=min_split,min_samples_leaf=min_leaf,max_features=feat) #, oob_score=obs)
                            print("Criterion: "+str(crit)+", N_estimators: "+str(n_est)+", max_depth: "+str(depth)+", min_samples_split: "+str(min_split)+", min_samples_leaf: "+str(min_leaf)+", max_features: "+str(feat))#+",oobs_score"+str(obs))
                            score = kfold_cross_validation(df,ran_for,10,features,target)
                            #result="Criterion:,"+str(crit)+", Splitter:,"+str(split)+", max_depth:,"+str(depth)+", min_samples_split:,"+str(min_split)+", min_samples_leaf:,"+str(min_leaf)+", max_features:,"+str(feat)+",score:,"+str(score)+",\n"
                            result="Criterion:,"+str(crit)+", N_estimators:,"+str(n_est)+", max_depth:,"+str(depth)+", min_samples_split:,"+str(min_split)+", min_samples_leaf:,"+str(min_leaf)+", max_features:,"+str(feat)+",accuracy:,"+str(score[0])+",precision:,"+str(score[1])+",recall:,"+str(score[2])+",f1score:,"+str(score[3])+",\n"
                            file1 = open("RF_paramtunning_class.txt","a") #Append Only (‘a’) : Open the file for writing. The file is created if it does not exist. The handle is positioned at the end of the file. The data being written will be inserted at the end, after the existing data.
                            file1.write(result)
                            file1.close()
                            #if(score>max_score):
                            if(score[0]>max_score):#here we use accuracy, but we can change it for example to RMES using score[4]
                                max_score=score[0]
                                max_score_param = "Criterion: "+str(crit)+", N_estimators: "+str(n_est)+", max_depth: "+str(depth)+", min_samples_split: "+str(min_split)+", min_samples_leaf: "+str(min_leaf)+", max_features: "+str(feat)
    best = [max_score,max_score_param]
    return(best)

best_RF = RF_param_tunning(ds_train,features,"fate")
#[0.9635294117647059,
# 'Criterion: gini, N_estimators: 100, max_depth: 10, min_samples_split: 6, min_samples_leaf: 2, max_features: None']
#creating the final tree with the parameters obtained from parameter tunning
RF_final = RandomForestClassifier(n_estimators = 100 , criterion = 'gini' , max_depth=10, min_samples_split=6, min_samples_leaf=2,max_features=None)

x_train = ds_train[features]
y_train = ds_train["fate"]

x_test = ds_test[features]
y_test = ds_test["fate"]

RF_final.fit(x_train, y_train) #training
y_pred_RF = RF_final.predict(x_test)

#...........................RF: Perfomance measures .....................
print("Accuracy:")
accuracy_score(y_test, y_pred_RF)
#0.9533333333333334
print("Precision:")
precision_score(y_test, y_pred_RF, average="weighted") #labels=np.unique(y_pred) can be added to calculate the measure only for the labels that have predicted samples
#0.9532691944916053

print("Recall:")
recall_score(y_test, y_pred_RF, average="weighted")
#0.9533333333333334

print("F1-Score:")
f1_score(y_test, y_pred_RF, average="weighted")
#0.9532565609087347


#---------------------------------------------------------------------------------
#------------------------------------ANN with GS----------------------------------
#---------------------------------------------------------------------------------

from sklearn.neural_network import MLPClassifier


hidden_layer_sizes = [(5,),(10,),(12,),(14,),(16,),(20,),(50,),(100,),
                          (5,2),(10,5),(12,6),(14,7),(16,8),(20,10),(50,25),(100,50),
                          (5,5),(10,10),(12,12),(14,14),(16,16),(20,20),(50,50),(100,100)]
    activation  = ["relu","identity", "logistic", "tanh"]
    solver = ["lbfgs", "sgd", "adam"]
    alpha = [0.00001,0.0001,0.0002,0.0003,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1]
    max_iter = [100,150,200,250,300,350,400,500,600,700,800,900,1000,1500,2000,5000]
    early_stopping  = [True,False]
    learning_rate = ["constant", "invscaling", "adaptive"]


ANN_GS = MLPClassifier()
parameters_ANN={'hidden_layer_sizes' : [(5,),(10,)],'max_iter': [100] ,'activation': ["relu","identity", "logistic", "tanh"],
            'early_stopping': [True],'learning_rate': ["constant", "adaptive"]}
#parameters={'min_samples_split' : range(18,500,10),'max_depth': range(1,20,2)}
best_ANN = GridSearchCV(ANN_GS,parameters_ANN)
best_ANN.fit(x_train, y_train)
y_pred_ANN_GS = best_ANN.predict(x_test)

#...........................ANN with GS: Perfomance measures .....................

print("Accuracy:")
accuracy_score(y_test, y_pred_ANN_GS)
#0.9333333333333333
print("Precision:")
precision_score(y_test, y_pred_DT_GS, average="weighted") #labels=np.unique(y_pred) can be added to calculate the measure only for the labels that have predicted samples
#0.9374031007751938
print("Recall:")
recall_score(y_test, y_pred_DT_GS, average="weighted")
#0.9333333333333333
print("F1-Score:")
f1_score(y_test, y_pred_DT_GS, average="weighted")
#0.9338490820900105


#---------------------------------------------------------------------------------
#-------------------------------------  ANN  -------------------------------------
#---------------------------------------------------------------------------------
def ANN_param_tunning_classification(df,features,target):
    max_score = 0
    score = 0
    #hidden_layer_sizes is a tuple of size (n_layers -2) because input and output layers don't count
    #so: (7,) is one hidden layer of 7 units, and (7,7) is 2 hidden layers of 7 units and (7,7,7) is 3 hidden
    #layers with 7 units
    hidden_layer_sizes = [(5,),(10,)]
    activation  = ["relu","identity", "logistic", "tanh"]
    #solver = ["lbfgs", "sgd", "adam"]
    #alpha = [0.00001,0.0001,0.0002,0.0003,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1]
    max_iter = [100,500]
    early_stopping  = [True]
    learning_rate = ["constant", "invscaling", "adaptive"]
    for hid in hidden_layer_sizes:
        for act in activation:
            for lr in learning_rate:
                for es in early_stopping:
                    for max_it in max_iter:
                        ANN = MLPClassifier(hidden_layer_sizes= hid,activation =act ,max_iter=max_it,early_stopping=es,learning_rate=lr)
                        print("hidden_layer_sizes"+str(hid)+"Activation: "+str(act)+", max_iter: "+str(max_it)+", early_stopping: "+str(es)+", learning_rate: "+str(lr))
                        score = kfold_cross_validation(df,ANN,10,features,target)
                        #result="Criterion:,"+str(crit)+", Splitter:,"+str(split)+", max_depth:,"+str(depth)+", min_samples_split:,"+str(min_split)+", min_samples_leaf:,"+str(min_leaf)+", max_features:,"+str(feat)+",score:,"+str(score)+",\n"
                        result="hidden_layer_sizes"+str(hid)+"Activation:,"+str(act)+", max_iter:,"+str(max_it)+", early_stopping:,"+str(es)+", learning_rate:,"+str(lr)+",accuracy:,"+str(score[0])+",precision:,"+str(score[1])+",recall:,"+str(score[2])+",f1score:,"+str(score[3])+",\n"
                        file1 = open("ANN_param_tunning_class.txt","a") #Append Only (‘a’) : Open the file for writing. The file is created if it does not exist. The handle is positioned at the end of the file. The data being written will be inserted at the end, after the existing data.
                        file1.write(result)
                        file1.close()
                        print("hidden_layer_sizes"+str(hid)+"Activation:,"+str(act)+", max_iter:,"+str(max_it)+", early_stopping:,"+str(es)+", learning_rate:,"+str(lr))
                        #if(score>max_score):
                        if(score[0]>max_score):#here we use accuracy, but we can change it for example to f1score using score[3]
                            max_score=score[0]
                            max_score_param = "hidden_layer_sizes"+str(hid)+"Activation:,"+str(act)+", max_iter:,"+str(max_it)+", early_stopping:,"+str(es)+", learning_rate:,"+str(lr)
    best = [max_score,max_score_param]
    return(best)

best_ANN = ANN_param_tunning_classification(ds_train,features,"fate")
#[0.6447058823529412,
# 'hidden_layer_sizes(5,)Activation:,logistic, max_iter:,500, early_stopping:,True, learning_rate:,adaptive']


#---------------------------------------------------------------------------------------------
#-----------------------------Final function to make predictions -----------------------------
#---------------------------------------------------------------------------------------------

def guess_kennys_fate():
    import pickle
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier

    # load the model
    RF_final = pickle.load(open('RF_class_model.sav', 'rb')) 
        
    keep = True
    while keep:   
        #inicialicing flags
        coordinate_x_in = False
        coordinate_y_in = False
        age_in = False
        horns_length_in = False
        height_in = False
        weight_in = False
        fur_color_in = False
        keep_in = False
        name = input("What's the koodoo's name?")
        while not(coordinate_x_in) :
            try :
                coordinate_x = int(input("What's the x coordinate of the koodoo's position?"))
                coordinate_x_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
        
        while not(coordinate_y_in) :
            try :
                coordinate_y = int(input("What's the y coordinate of the koodoo's position?"))
                coordinate_y_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
            
        while not(age_in) :
            try :
                age = int(input("How old is the koodoo?"))
                age_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
            
        while not(horns_length_in) :
            try :
                horns_length = int(input("What's the length of the koodoo's horns?"))
                horns_length_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
        
        while not(height_in) :
            try :
                height = int(input("What's the koodoo's height?"))
                height_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
        
        while not(weight_in) :
            try :
                weight = int(input("What's the koodoo's weight?"))
                weight_in = True
            except ValueError :
                # incorrect format, report and ask again.
                print("Please input an integer")
                continue
        while not(fur_color_in):
            fur_color = input("What color is the koodoo's fur? Input D if Dark brown or L if Light brown.")
            if(fur_color=="D"):
                fur_color=0
                fur_color_in = True
            elif(fur_color=="L"):
                fur_color=1
                fur_color_in = True
            else:
                print("Please input D or L")
        
        
        data_row = {
                'name': [name],
                'coordinate_x': [coordinate_x],
                'coordinate_y': [coordinate_y],
                'age': [age],
                'horns_length': [horns_length],
                'height': [height],
                'weight': [weight],
                'fur_color': [fur_color]
            }
        
        row = pd.DataFrame(data_row)
        features = ['coordinate_x','coordinate_y','age','horns_length', 'height','weight','fur_color'] 
    
        y_pred_game = RF_final.predict(row[features])
        if(y_pred_game[0] == 0):    
            print('Your koodoo '+ name +'... DIES :’(')
        else:
            print('Your koodoo '+ name +'... SURVIVES!! :D')
        while not(keep_in):
            keep = input('Keep predicting?(Y/N)')
            if(keep=="Y"):
                keep=True
                keep_in = True
            elif(keep=="N"):
                keep=False
                keep_in = True
            else:
                print("Please input Y or N")

#----------  Just a quick way to declare all the data and create the chosen model
ds = pd.read_csv('Koodoo_survival_statistics.csv')
#Transforming categorical attributes:
fate_to_num = {"fate":     {"Died": 0, "Survived": 1}}
ds.replace(fate_to_num, inplace=True)
fur_color_to_num = {"fur_color":     {"Light Brown": 1, "Dark Brown": 0}}
ds.replace(fur_color_to_num, inplace=True)

#defining features
features = ['coordinate_x','coordinate_y','age','horns_length', 'height','weight','fur_color'] 

#splitting data in training and test set
ds_train, ds_test = train_test_split(ds, test_size=0.15)

RF_final = RandomForestClassifier(n_estimators = 100 , criterion = 'gini' , max_depth=10, min_samples_split=6, min_samples_leaf=2,max_features=None)

x_train = ds_train[features]
y_train = ds_train["fate"]

x_test = ds_test[features]
y_test = ds_test["fate"]

RF_final.fit(x_train, y_train) #training
y_pred_RF = RF_final.predict(x_test)
print("Accuracy:")
accuracy_score(y_test, y_pred_RF)
#0.96
print("Precision:")
precision_score(y_test, y_pred_RF, average="weighted") #labels=np.unique(y_pred) can be added to calculate the measure only for the labels that have predicted samples
#0.96209726443769

print("Recall:")
recall_score(y_test, y_pred_RF, average="weighted")
#0.96

print("F1-Score:")
f1_score(y_test, y_pred_RF, average="weighted")
#0.960324074074074

# Extract the small tree
one_tree = RF_final.estimators_[5]
# Save the tree as a png image
show_tree(one_tree, features,'one_tree.png')


#Get numerical feature importances
importances = list(RF_final.feature_importances_)
# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]
# Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

import pickle

pickle.dump(RF_final, open('RF_class_model.sav', 'wb'))
